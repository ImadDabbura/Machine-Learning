{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: Georgia; font-size:3em;color:#2462C0; font-style:bold\">\n",
    "Multi-class Classification and Neural Networks\n",
    "</h1><br>\n",
    "\n",
    "For this exercise we'll use logistic regression to recognize hand-written digits (0 to 9). In addition, we'll implement neural network on the same problem using given weights that were trained on the same problem. The recognition of hand-written digits is a multi-class classification problem with $k = 10$ classes, i.e $y^i \\in \\{1, 2, 3, ..., 10\\}$. As a result, to use logistic regression, we have to fit 10 classifiers using  **one-vs-all** technique. Therefore, for each class we'll build a classifier where that class would be the *positive* class and all other classes would be the *negative* class. After fitting the 10 classifiers, we use the 10 models in predicting each training example $x^i$ and pick the class that has the highest probability from those 10 models; i.e would end up with column vector $h_\\theta(x^i) \\in \\mathbb{R}^k$.\n",
    "\n",
    "Let's get started by imporitng the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries and set up notebook global style\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat, whosmat # load Matlab files and inspect objects in those files before loading them\n",
    "import scipy.optimize as opt\n",
    " \n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('fivethirtyeight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Functions\n",
    "</h2><br>\n",
    "We'll write the functions needed to solve regularized logistic regression and implement Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------- Define fn needed to solve logistic regression problem -------------------#\n",
    "# Define Sigmoid fn\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# compute cost of regularized logistic regression\n",
    "def costReg(theta, X, y, lambda_):\n",
    "    '''\n",
    "    theta: parameters array\n",
    "    X: features matrix m x (n + 1)\n",
    "    y: target variable column vector m x 1\n",
    "    lambda: shrinkage parameter --> scalar\n",
    "    Return: cost with shrinkage penalty added --> scalar\n",
    "    '''\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    z = X.dot(theta)\n",
    "    hyp = sigmoid(z) # hypothesis\n",
    "    error = np.multiply(y, np.log(hyp)) + np.multiply(1 - y, np.log(1 - hyp))\n",
    "    shrinkage_penalty = (lambda_ /(2 * m)) * (np.power(theta[1:, :], 2))\n",
    "    cost = - (1 / m) * np.sum(error) + np.sum(shrinkage_penalty)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# compute the gradients of regularized logistic regression\n",
    "def gradientReg(theta, X, y, lambda_):\n",
    "    '''\n",
    "    theta: parameters array\n",
    "    X: features matrix m x (n + 1)\n",
    "    y: target variable column vector m x 1\n",
    "    lambda: shrinkage parameter --> scalar\n",
    "    Return: gradient array\n",
    "    '''\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    z = X.dot(theta)\n",
    "    hyp = sigmoid(z)\n",
    "    error = hyp - y\n",
    "    grad = (1 / m) * (X.T.dot(error))\n",
    "    grad[1:, :] = grad[1:, :] + (lambda_ / m) * theta[1:, :]\n",
    "    \n",
    "    return grad.ravel()\n",
    "\n",
    "# define one-vs-all k classifier\n",
    "def oneVsAll(X, y, num_classes, lambda_):\n",
    "    '''\n",
    "    X: features matrix m x (n + 1)\n",
    "    y: target variable column vector m x 1\n",
    "    num_classes: scalar; number of classes at hand\n",
    "    lambda: shrinkage parameter --> scalar\n",
    "    Return: theta matrix k x (n + 1) where each row is a classifier and each column is a featute.\n",
    "            i.e each cell is a fauture weight for the classifier on the same row.\n",
    "    '''\n",
    "    # Initializes zero theta parameter and theta matrix\n",
    "    init_theta = np.zeros(X.shape[1])\n",
    "    # theta matrix will store theta parameters for all k models\n",
    "    theta_matrix = np.zeros((num_classes, X.shape[1]))\n",
    "    \n",
    "    for i in range(1, num_classes + 1):\n",
    "        # would be one for the i class and zero for all ther classes i.e one-vs-all\n",
    "        y_i = (y == i) * 1 \n",
    "        # compute theta\n",
    "        result = opt.fmin_tnc(func = costReg, x0 = init_theta, args = (X, y_i, lambda_), fprime = gradientReg)\n",
    "        # store theta parameters array in theta matrix\n",
    "        theta_matrix[i - 1] = result[0]\n",
    "    \n",
    "    return theta_matrix\n",
    "\n",
    " # define predict fn for one-vs-all\n",
    "def predictOneVsAll(theta, X):\n",
    "    '''\n",
    "    X: features matrix m x (n + 1)\n",
    "    theta: theta matrix k x (n + 1\n",
    "    Return: predictions column vector m x 1\n",
    "    '''\n",
    "    pred = X.dot(theta.T) # will leave us with matrix m * k where each column represents the probability that the ith\n",
    "                        # example belong to that class.\n",
    "    hyp = sigmoid(pred)\n",
    "    pred = (pred.argmax(axis = 1) + 1) # it will give us the index of the highest probability per row (column index) which\n",
    "                                 # will allow us to assign the class for the training example\n",
    "    return pred.reshape(-1, 1)\n",
    "\n",
    "# define accuracy fn\n",
    "def accuracy(predictions, y):\n",
    "    '''\n",
    "    predictions: predictions' class\n",
    "    y: Actual class for each m examples --> m x 1 column vector\n",
    "    Return: Accuracy rate --> scalar\n",
    "    '''\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    return 'accuracy = {:.2f}%'.format(accuracy * 100)\n",
    "\n",
    "#------------------------- Define fn needed to implement NN forward propagation -------------------------#\n",
    "# define the fn that uses given weights to implement the neural network from input layer to output layer\n",
    "# and predict the class of given examples\n",
    "def predict_nn(Theta1, Theta2, X):\n",
    "    '''\n",
    "    X: features matrix m x (n + 1)\n",
    "    Theta1: theta matrix 25 x (n + 1) --> weights used to get the second activation units\n",
    "    Theta1: theta matrix k x 26 --> weights used to get the second activation units\n",
    "    Return: predictions class column vector m x 1\n",
    "    '''\n",
    "    z_2 = X.dot(Theta1.T)\n",
    "    # apply the sigmoid fn to get the 2nd activation units\n",
    "    a_2 = sigmoid(z_2)\n",
    "    # add ones for the intercept\n",
    "    a_2 = np.insert(a_2, 0, np.ones(X.shape[0]), axis = 1)\n",
    "    z_3 = a_2.dot(Theta2.T)\n",
    "    a_3 = sigmoid(z_3) # Output layer which is column vector of k * 1 per training example (probability of\n",
    "                       # this example belongs to each class\n",
    "    pred = (a_3.argmax(axis = 1) + 1)  # it will give us the index of the highest probability per row (column index) which\n",
    "                                 # will allow us to assign the class for the training example. Note that we added one because\n",
    "                                 # python starts indexing at 1 and our data is has 0 as 10.\n",
    "    return pred.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Regularized Logistic Regression: Multi-class Classification\n",
    "</h2><br>\n",
    "Below is a refresher from logistic regression notebook.\n",
    "\n",
    "The hypothesis function is:\n",
    "$$P(y^i = 1/X,\\theta) = h_\\theta(X^i) = \\frac {1} {1+ \\exp^{-\\theta^{T}X^i}}$$\n",
    "\n",
    "$$\\Rightarrow\\Big(\\frac {P(y^i = 1/X,\\theta)} {1 - P(y^i = 1/X,\\theta)}\\Big) = \\exp^{\\theta^{T}X^i}$$\n",
    "\n",
    "$$\\Rightarrow log\\Big(\\frac {P(y^i = 1/X,\\theta)} {1 - P(y^i = 1/X,\\theta)}\\Big) = {\\theta^{T}X^i}$$\n",
    "\n",
    "The cost function will be:\n",
    "$$J(\\theta) = - \\frac {1} {m} \\sum_{i = 1}^{m} \\big\\{ y^ilog(h_\\theta(X^i)) + (1 - y^i)log(1 - h_\\theta(X^i))\\big\\} + \\frac {\\lambda} {2m} \\sum_{j = 1}^{n}\\theta_j^2$$\n",
    "Since we don't apply the regularization on the bias (intercept); therefore, we'll have two update equations for the gradient descent algorithm:\n",
    "$$\\theta_0 = \\theta_0 - \\alpha\\sum_{i = 1}^{m} (h_\\theta(X^i) - y^i)x_0^i$$\n",
    "$$\\theta_j = \\theta_j(1 - \\alpha\\frac{\\lambda}{m}) - \\alpha\\sum_{i = 1}^{m} (h_\\theta(X^i) - y^i)x_j^i\\quad;\\,j = 1, 2 , ..., n$$\n",
    "\n",
    "Finally, since it's *one-vs-all* classification, we'll have k hypothesis for each example:\n",
    "Train a logistic regression classifier $h_\\theta^i(X)$ for each class $i$ to predict the probability that $y = i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique classes we have: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "Below is a sample of 20 images of the digits we have:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAA1CAYAAAAOAZ9nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAwJbIdnY2NjYDAj+D64FrNCkushEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11128c518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "data = loadmat('../data/ex3data1.mat')\n",
    "y = data['y']\n",
    "X = data['X']\n",
    "# get the number of classes we have using unique method\n",
    "num_classes = len(np.unique(y))\n",
    "print('The unique classes we have: {}'.format(np.unique(y)))\n",
    "# randomly select 20 images to show them\n",
    "sample = np.random.choice(X.shape[0], 20)\n",
    "print('Below is a sample of 20 images of the digits we have:')\n",
    "plt.imshow(X[sample, :].reshape(-1, 20).T)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Matlab file contains two objects: \n",
    "* $X$: feature matrix with 400 features. Each example is a 20 pixels x 20 pixels image $\\rightarrow$ 400 features.\n",
    "* $y$: target variable. Note that 0 is 10 in the data for indexing purposes.\n",
    "* $m = 5000$ training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy = 96.46%'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding columns of ones for the intercept\n",
    "X = np.insert(X, 0, np.ones(X.shape[0]), axis = 1)\n",
    "# Run the OneVsAll to get all models\n",
    "lambda_ = 0.1\n",
    "all_theta = oneVsAll(X, y, num_classes, lambda_)\n",
    "# predict the training examples class\n",
    "pred = predictOneVsAll(all_theta, X)\n",
    "accuracy(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of using 10 clasifiers is close to 96.5%. We could've tried adding some non-linearities to the model as well as tuning the hyperparameter $\\lambda$ using cross validation to get the most out of the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Neural Network\n",
    "</h2><br>\n",
    "Even though we can add non-linearity to logistic regression, it'll be very computationally expensive and it still can't be as complex as *nueral network* in terms of non-linearity. In this part of the exercise, we'll implement a neural network to recognize handwritten digits using the same training set used in the previous exercise. The neural network will be able to represent complex models that form non-linear hypotheses. we'll be using parameters from a neural network that have already been trained. So we'll implement the feedforward propagation algorithm to use our weights for prediction.\n",
    "\n",
    "Let's load the weights matrices and write a function to use them in predicting the class for the training images we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the data\n",
    "# load trained parameters\n",
    "data = loadmat('../data/ex3weights.mat')\n",
    "Theta1 = data['Theta1']\n",
    "Theta2 = data['Theta2']\n",
    "Theta1.shape, Theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy = 97.52%'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_nn = predict_nn(Theta1, Theta2, X)\n",
    "accuracy(prediction_nn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the trained *Neural Network* yields an accuracy close to 98% which proves that neural network handles complexity in models better than any other parametric model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Conclusion\n",
    "</h2><br>\n",
    "A few takeaway points:\n",
    "* We didn't tune shrinkage parameter to pick the soft spot of *bias-variance* trade-off that may've helped us in improving the accuracy rate.\n",
    "    * We could've also add non-linearity to the features but it may become very complex so fast since adding $2^{nd}$ degree polynomial will grow at ${O}(n^2)$.\n",
    "* Applying the weights matrices to get $h_\\Theta(X)$ is called **Forward Propagation** which is what we've done in the $2^{nd}$ part of the exercise.\n",
    "* **Backpropagation** starts at the $error = h_\\Theta(X) - y$ and go back to derive the gradiant of each activation layer to see which part was reponsible for the biggest error and change the direction that has the steepest move from current weight on $J(\\Theta)$. This will continue until we reach the local/global minimum where all gradients would be **zero**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
