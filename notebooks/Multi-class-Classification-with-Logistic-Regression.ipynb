{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: Georgia; font-size:3em;color:#2462C0; font-style:bold\">\n",
    "Multi-class Classification with Logistic Regression\n",
    "</h1><br>\n",
    "\n",
    "For this exercise we'll use logistic regression to recognize hand-written digits (0 to 9). The recognition of hand-written digits is a multi-class classification problem with $k = 10$ classes, i.e $y^i \\in \\{1, 2, 3, ..., 10\\}$. As a result, to use logistic regression, we have to fit 10 classifiers using  **one-vs-all** technique. Therefore, for each class we'll build a classifier where that class would be the *positive* class and all other classes would be the *negative* class. After fitting the 10 classifiers, we use the 10 models in predicting each training example $x^i$ and pick the class that has the highest probability from those 10 models; i.e would end up with column vector $h_\\theta(x^i) \\in \\mathbb{R}^k$.\n",
    "\n",
    "Let's get started by imporitng the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries and set up notebook global style\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat, whosmat # load Matlab files and inspect objects in those files before loading them\n",
    "import scipy.optimize as opt\n",
    " \n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('fivethirtyeight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Functions\n",
    "</h2><br>\n",
    "We'll write the functions needed to solve regularized logistic regression and implement Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define sigmoid fn\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "# Compute cost of regularized logistic regression\n",
    "def cost_reg(theta, X, y, lambda_):\n",
    "    '''\n",
    "    theta: parameters array\n",
    "    X: features matrix m x (n + 1)\n",
    "    y: target variable column vector m x 1\n",
    "    lambda: shrinkage parameter --> scalar\n",
    "    Return: cost with shrinkage penalty added --> scalar\n",
    "    '''\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    z = X.dot(theta)\n",
    "    hyp = sigmoid(z) # hypothesis\n",
    "    error = np.multiply(y, np.log(hyp)) + np.multiply(1 - y, np.log(1 - hyp))\n",
    "    shrinkage_penalty = (lambda_ /(2 * m)) * (np.power(theta[1:, :], 2))\n",
    "    cost = - (1 / m) * np.sum(error) + np.sum(shrinkage_penalty)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "# Compute the gradients of regularized logistic regression\n",
    "def gradient_reg(theta, X, y, lambda_):\n",
    "    '''\n",
    "    theta: parameters array\n",
    "    X: features matrix m x (n + 1)\n",
    "    y: target variable column vector m x 1\n",
    "    lambda: shrinkage parameter --> scalar\n",
    "    Return: gradient array\n",
    "    '''\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    z = X.dot(theta)\n",
    "    hyp = sigmoid(z)\n",
    "    error = hyp - y\n",
    "    grad = (1 / m) * (X.T.dot(error))\n",
    "    grad[1:, :] = grad[1:, :] + (lambda_ / m) * theta[1:, :]\n",
    "    \n",
    "    return grad.ravel()\n",
    "\n",
    "\n",
    "# Define one-vs-all k classifier\n",
    "def one_vs_all(X, y, num_classes, lambda_):\n",
    "    '''\n",
    "    X: features matrix m x (n + 1)\n",
    "    y: target variable column vector m x 1\n",
    "    num_classes: scalar; number of classes at hand\n",
    "    lambda: shrinkage parameter --> scalar\n",
    "    Return: theta matrix k x (n + 1) where each row is a classifier and each column is a featute.\n",
    "            i.e. each cell is a fauture weight for the classifier on the same row.\n",
    "    '''\n",
    "    # Initializes zero theta parameter and theta matrix\n",
    "    init_theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    # Theta matrix will store theta parameters for all k models\n",
    "    theta_matrix = np.zeros((num_classes, X.shape[1]))\n",
    "    \n",
    "    for i in range(1, num_classes + 1):\n",
    "        # Would be 1 for the i class and 0 for all ther classes i.e one-vs-all\n",
    "        y_i = (y == i) * 1 \n",
    "        \n",
    "        # Compute theta\n",
    "        result = opt.fmin_tnc(\n",
    "            func = cost_reg, x0=init_theta, args=(X, y_i, lambda_), fprime=gradient_reg)\n",
    "        \n",
    "        # Store theta parameters array in theta matrix\n",
    "        theta_matrix[i - 1] = result[0]\n",
    "    \n",
    "    return theta_matrix\n",
    "\n",
    "\n",
    "# Define predict fn for one-vs-all\n",
    "def predict_one_vs_all(theta, X):\n",
    "    '''\n",
    "    X: features matrix m x (n + 1)\n",
    "    theta: theta matrix k x (n + 1\n",
    "    Return: predictions column vector m x 1\n",
    "    '''\n",
    "    pred = X.dot(theta.T) # will leave us with matrix m * k where each column represents\n",
    "                         # the probability that the ith example belong to that class.\n",
    "    hyp = sigmoid(pred)\n",
    "    pred = (pred.argmax(axis = 1) + 1) # it will give us the index of the highest probability\n",
    "                                      # per row (column index) which will allow us to assign\n",
    "                                      # the class for the training example\n",
    "    return pred.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Define accuracy fn\n",
    "def accuracy(predictions, y):\n",
    "    '''\n",
    "    predictions: predictions' class\n",
    "    y: Actual class for each m examples --> m x 1 column vector\n",
    "    Return: Accuracy rate --> scalar\n",
    "    '''\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    return f\"accuracy = {accuracy * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Regularized Logistic Regression: Multi-class Classification\n",
    "</h2><br>\n",
    "Below is a refresher from logistic regression notebook.\n",
    "\n",
    "The hypothesis function is:\n",
    "$$P(y^i = 1/X,\\theta) = h_\\theta(X^i) = \\frac {1} {1+ \\exp^{-\\theta^{T}X^i}}$$\n",
    "\n",
    "$$\\Rightarrow\\Big(\\frac {P(y^i = 1/X,\\theta)} {1 - P(y^i = 1/X,\\theta)}\\Big) = \\exp^{\\theta^{T}X^i}$$\n",
    "\n",
    "$$\\Rightarrow log\\Big(\\frac {P(y^i = 1/X,\\theta)} {1 - P(y^i = 1/X,\\theta)}\\Big) = {\\theta^{T}X^i}$$\n",
    "\n",
    "The cost function will be:\n",
    "$$J(\\theta) = - \\frac {1} {m} \\sum_{i = 1}^{m} \\big\\{ y^ilog(h_\\theta(X^i)) + (1 - y^i)log(1 - h_\\theta(X^i))\\big\\} + \\frac {\\lambda} {2m} \\sum_{j = 1}^{n}\\theta_j^2$$\n",
    "Since we don't apply the regularization on the bias (intercept); therefore, we'll have two update equations for the gradient descent algorithm:\n",
    "$$\\theta_0 = \\theta_0 - \\alpha\\sum_{i = 1}^{m} (h_\\theta(X^i) - y^i)x_0^i$$\n",
    "$$\\theta_j = \\theta_j(1 - \\alpha\\frac{\\lambda}{m}) - \\alpha\\sum_{i = 1}^{m} (h_\\theta(X^i) - y^i)x_j^i\\quad;\\,j = 1, 2 , ..., n$$\n",
    "\n",
    "Finally, since it's *one-vs-all* classification, we'll have k hypothesis for each example:\n",
    "Train a logistic regression classifier $h_\\theta^i(X)$ for each class $i$ to predict the probability that $y = i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique classes we have: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "Below is a sample of 20 images of the digits we have:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAA1CAYAAADs3YQeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB3k4qKNoJ6QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112ff8400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "data = loadmat('../data/ex3data1.mat')\n",
    "y = data['y']\n",
    "X = data['X']\n",
    "\n",
    "# Get the number of classes we have using unique method\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"The unique classes we have: {np.unique(y)}\")\n",
    "\n",
    "# randomly select 20 images to show them\n",
    "sample = np.random.choice(X.shape[0], 20)\n",
    "print(\"Below is a sample of 20 images of the digits we have:\")\n",
    "plt.imshow(X[sample, :].reshape(-1, 20).T)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Matlab file contains two objects: \n",
    "* $X$: feature matrix with 400 features. Each example is a 20 pixels x 20 pixels image $\\rightarrow$ 400 features.\n",
    "* $y$: target variable. Note that 0 is 10 in the data for indexing purposes.\n",
    "* $m = 5000$ training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy = 96.46%'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding columns of ones for the intercept\n",
    "X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "\n",
    "# Run the OneVsAll to get all models\n",
    "lambda_ = 0.1\n",
    "all_theta = one_vs_all(X, y, num_classes, lambda_)\n",
    "\n",
    "# Predict the training examples class\n",
    "pred = predict_one_vs_all(all_theta, X)\n",
    "accuracy(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of using 10 clasifiers is close to 96.46%. We could've tried adding some non-linearities to the model as well as tuning the hyperparameter $\\lambda$ using cross validation to get the most out of the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Conclusion\n",
    "</h2><br>\n",
    "A few takeaway points:\n",
    "* We didn't tune shrinkage parameter to pick the soft spot of *bias-variance* trade-off that may've helped us in improving the accuracy rate.\n",
    "    * We could've also add non-linearity to the features but it may become very complex so fast since adding $2^{nd}$ degree polynomial will grow at ${O}(n^2)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
